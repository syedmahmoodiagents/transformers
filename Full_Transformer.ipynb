{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPazXKR1Mj28WP3KUs/WO1C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syedmahmoodiagents/transformers/blob/main/Full_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "tCimTTvFsN0m"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "PMZSR9uCsKlH"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "    def __init__(self, embedding_dim, block_size, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        sm = q @ k.transpose(-2, -1) / (C ** 0.5)\n",
        "        msk = sm.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        sft = F.softmax(msk, dim=-1)\n",
        "        v = self.value(x)\n",
        "        out = sft @ v\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim, block_size, num_heads):\n",
        "        super().__init__()\n",
        "        head_size = embedding_dim // num_heads\n",
        "        self.heads = nn.ModuleList([SelfAttentionHead(embedding_dim, block_size, head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * head_size, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return self.proj(out)\n"
      ],
      "metadata": {
        "id": "R35s4kniLZJd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "Xj4sW_O9LbWX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, embedding_dim, block_size, n_heads):\n",
        "        super().__init__()\n",
        "        self.sa = MultiHeadAttention(embedding_dim, block_size, n_heads)\n",
        "        self.ffwd = FeedForward(embedding_dim)\n",
        "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
        "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZcyMGstkLdNr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, block_size, n_heads, n_layers):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(src_vocab_size, embedding_dim)\n",
        "        self.position_embedding = nn.Embedding(block_size, embedding_dim)\n",
        "        self.blocks = nn.Sequential(*[Block(embedding_dim, block_size, n_heads) for _ in range(n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
        "        self.lm_head = nn.Linear(embedding_dim, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss"
      ],
      "metadata": {
        "id": "Hb-aPYmvEiOg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = [\n",
        "    (\"i am hungry\", \"ik ben hongerig\"),\n",
        "    (\"he is tired\", \"hij is moe\"),\n",
        "    (\"she is happy\", \"zij is blij\"),\n",
        "    (\"she is not tired\", \"zij is niet moe\"),\n",
        "    (\"he is not happy\", \"hij is niet blij\"),\n",
        "]"
      ],
      "metadata": {
        "id": "HRSwSlxquFfx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    return text.lower().split()"
      ],
      "metadata": {
        "id": "Y4X9yBMcEQ1r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_vocab(sentences):\n",
        "    vocab = {\"<PAD>\": 0, \"<BOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "    for sentence in sentences:\n",
        "        for token in tokenize(sentence):\n",
        "            if token not in vocab:\n",
        "                vocab[token] = len(vocab)\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "jgUsPlw3Ebv-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentences = [src for src, _ in pairs]\n",
        "tgt_sentences = [tgt for _, tgt in pairs]"
      ],
      "metadata": {
        "id": "dT-3a4wTEXGe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP6DdGMTE3eL",
        "outputId": "33990bbb-0f3d-413d-ab73-fb7bef3b7a14"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i am hungry',\n",
              " 'he is tired',\n",
              " 'she is happy',\n",
              " 'she is not tired',\n",
              " 'he is not happy']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab = build_vocab(src_sentences)\n",
        "tgt_vocab = build_vocab(tgt_sentences)"
      ],
      "metadata": {
        "id": "87Z5GJf4EcYn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnx62AYQE8Rh",
        "outputId": "fbf91fc1-183b-4c2f-f445-369c64d2e2d1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<PAD>': 0,\n",
              " '<BOS>': 1,\n",
              " '<EOS>': 2,\n",
              " '<UNK>': 3,\n",
              " 'i': 4,\n",
              " 'am': 5,\n",
              " 'hungry': 6,\n",
              " 'he': 7,\n",
              " 'is': 8,\n",
              " 'tired': 9,\n",
              " 'she': 10,\n",
              " 'happy': 11,\n",
              " 'not': 12}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inv_tgt_vocab = {idx: tok for tok, idx in tgt_vocab.items()}"
      ],
      "metadata": {
        "id": "P6Jxt6RZEfmC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inv_tgt_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLK3xpIaFDrc",
        "outputId": "c98a6829-0228-438e-f5fb-8fef28fa5c8f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '<PAD>',\n",
              " 1: '<BOS>',\n",
              " 2: '<EOS>',\n",
              " 3: '<UNK>',\n",
              " 4: 'ik',\n",
              " 5: 'ben',\n",
              " 6: 'hongerig',\n",
              " 7: 'hij',\n",
              " 8: 'is',\n",
              " 9: 'moe',\n",
              " 10: 'zij',\n",
              " 11: 'blij',\n",
              " 12: 'niet'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de9e7efe"
      },
      "source": [
        "src_vocab_size = len(src_vocab)\n",
        "tgt_vocab_size = len(tgt_vocab)\n",
        "embedding_dim = 64\n",
        "block_size = 8\n",
        "n_heads = 4\n",
        "n_layers = 2\n",
        "learning_rate = 1e-3\n",
        "max_iters = 1000"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeb589a8",
        "outputId": "8af615a5-405b-4968-8ef0-a0c579280108"
      },
      "source": [
        "def prepare_data(src_sentences, tgt_sentences, src_vocab, tgt_vocab, block_size):\n",
        "    src_data = []\n",
        "    tgt_data = []\n",
        "\n",
        "    for src_s, tgt_s in zip(src_sentences, tgt_sentences):\n",
        "        # Process source sentence\n",
        "        src_tokens = tokenize(src_s)\n",
        "        src_indexed = [src_vocab.get(token, src_vocab['<UNK>']) for token in src_tokens]\n",
        "\n",
        "        # Pad source sequence\n",
        "        if len(src_indexed) > block_size:\n",
        "            src_indexed = src_indexed[:block_size]\n",
        "        else:\n",
        "            src_indexed = src_indexed + [src_vocab['<PAD>']] * (block_size - len(src_indexed))\n",
        "        src_data.append(src_indexed)\n",
        "\n",
        "        # Process target sentence\n",
        "        tgt_tokens = tokenize(tgt_s)\n",
        "        # Add BOS and EOS tokens\n",
        "        tgt_indexed = [tgt_vocab['<BOS>']] + [tgt_vocab.get(token, tgt_vocab['<UNK>']) for token in tgt_tokens] + [tgt_vocab['<EOS>']]\n",
        "\n",
        "        # Pad target sequence\n",
        "        if len(tgt_indexed) > block_size:\n",
        "            tgt_indexed = tgt_indexed[:block_size]\n",
        "        else:\n",
        "            tgt_indexed = tgt_indexed + [tgt_vocab['<PAD>']] * (block_size - len(tgt_indexed))\n",
        "        tgt_data.append(tgt_indexed)\n",
        "\n",
        "    src_tensor = torch.tensor(src_data, dtype=torch.long)\n",
        "    tgt_tensor = torch.tensor(tgt_data, dtype=torch.long)\n",
        "\n",
        "    return src_tensor, tgt_tensor\n",
        "\n",
        "print(\"The `prepare_data` function has been defined.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The `prepare_data` function has been defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d85b33dd",
        "outputId": "be2f67e7-5336-4cd3-ea7b-5457ff964ff5"
      },
      "source": [
        "src_tensor, tgt_tensor = prepare_data(src_sentences, tgt_sentences, src_vocab, tgt_vocab, block_size)\n",
        "\n",
        "print(f\"Source Tensor Shape: {src_tensor.shape}\")\n",
        "print(f\"Target Tensor Shape: {tgt_tensor.shape}\")\n",
        "print(\"Example Source Tensor (first row):\", src_tensor[0])\n",
        "print(\"Example Target Tensor (first row):\", tgt_tensor[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Tensor Shape: torch.Size([5, 8])\n",
            "Target Tensor Shape: torch.Size([5, 8])\n",
            "Example Source Tensor (first row): tensor([4, 5, 6, 0, 0, 0, 0, 0])\n",
            "Example Target Tensor (first row): tensor([1, 4, 5, 6, 2, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7477f7e8",
        "outputId": "ab4571c0-b5a6-41f9-d8c2-a10600c6f824"
      },
      "source": [
        "model = Transformer(src_vocab_size, tgt_vocab_size, embedding_dim, block_size, n_heads, n_layers)\n",
        "print(\"Transformer model instantiated.\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer model instantiated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15ef043a",
        "outputId": "d31f76b6-3be8-4eb9-ab35-cc8d2c81c29e"
      },
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "print(f\"Optimizer initialized with learning rate: {learning_rate}\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer initialized with learning rate: 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40f9648b",
        "outputId": "57ff2a76-1516-4531-803d-c050a5c71dd4"
      },
      "source": [
        "for iter in range(max_iters):\n",
        "    # Forward pass\n",
        "    logits, loss = model(src_tensor, tgt_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if iter % 100 == 0:\n",
        "        print(f\"Iteration {iter}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 2.5861\n",
            "Iteration 100, Loss: 0.0146\n",
            "Iteration 200, Loss: 0.0057\n",
            "Iteration 300, Loss: 0.0032\n",
            "Iteration 400, Loss: 0.0021\n",
            "Iteration 500, Loss: 0.0015\n",
            "Iteration 600, Loss: 0.0011\n",
            "Iteration 700, Loss: 0.0008\n",
            "Iteration 800, Loss: 0.0007\n",
            "Iteration 900, Loss: 0.0006\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3b813cd"
      },
      "source": [
        "def translate_sentence(model, sentence, src_vocab, tgt_vocab, inv_tgt_vocab, block_size, device='cpu'):\n",
        "    model.eval()\n",
        "    tokens = tokenize(sentence)\n",
        "    indexed = [src_vocab.get(token, src_vocab['<UNK>']) for token in tokens]\n",
        "\n",
        "    # Pad source sentence to block_size\n",
        "    if len(indexed) > block_size:\n",
        "        indexed = indexed[:block_size]\n",
        "    else:\n",
        "        indexed = indexed + [src_vocab['<PAD>']] * (block_size - len(indexed))\n",
        "\n",
        "    src_input = torch.tensor([indexed], dtype=torch.long, device=device)\n",
        "\n",
        "    translated_tokens = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Call the model once with src_input and targets=None for direct prediction\n",
        "        logits, _ = model(src_input, targets=None)  # Logits should now be (1, block_size, tgt_vocab_size)\n",
        "\n",
        "        # Iterate through the output positions to get the predicted tokens\n",
        "        for i in range(block_size):\n",
        "            prediction_logits = logits[0, i, :]  # Get logits for the current output position\n",
        "            probs = F.softmax(prediction_logits, dim=-1)\n",
        "            next_token_id = torch.argmax(probs, dim=-1).item()\n",
        "\n",
        "            # Stop decoding if an EOS or PAD token is predicted\n",
        "            if next_token_id == tgt_vocab['<EOS>']:\n",
        "                break\n",
        "            if next_token_id == tgt_vocab['<PAD>']:\n",
        "                break\n",
        "            # Skip BOS token if it's predicted as the first token, as it's an artifact of training target sequences\n",
        "            if next_token_id == tgt_vocab['<BOS>'] and i == 0:\n",
        "                continue\n",
        "\n",
        "            translated_tokens.append(inv_tgt_vocab[next_token_id])\n",
        "\n",
        "    # model.train()\n",
        "    return ' '.join(translated_tokens)\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74bfcb2f",
        "outputId": "a255ecaa-cae5-4983-91a3-3e369a37afbc"
      },
      "source": [
        "english_sentence = \"i am happy\"\n",
        "dutch_translation = translate_sentence(model, english_sentence, src_vocab, tgt_vocab, inv_tgt_vocab, block_size)\n",
        "\n",
        "print(f\"English: {english_sentence}\")\n",
        "print(f\"Dutch Translation: {dutch_translation}\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: i am happy\n",
            "Dutch Translation: ik is blij\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7636375d",
        "outputId": "987ce175-cc17-4444-8d66-c538f4c64afd"
      },
      "source": [
        "english_sentence_2 = \"he is tired\"\n",
        "dutch_translation_2 = translate_sentence(model, english_sentence_2, src_vocab, tgt_vocab, inv_tgt_vocab, block_size)\n",
        "\n",
        "print(f\"English: {english_sentence_2}\")\n",
        "print(f\"Dutch Translation: {dutch_translation_2}\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: he is tired\n",
            "Dutch Translation: hij is moe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rXzu3pCpNIZm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}