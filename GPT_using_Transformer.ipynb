{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6VJ0Y2rQsnlhFlcJOghOg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syedmahmoodiagents/transformers/blob/main/GPT_using_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "O8N3g0TnFRJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMuENrmQFMBA"
      },
      "outputs": [],
      "source": [
        "class GPTBlock(nn.Module):\n",
        "    def __init__(self, embed_dim=768, num_heads=12, ffn_hidden=3072, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Causal Self-Attention\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        # Feed Forward\n",
        "        self.linear1 = nn.Linear(embed_dim, ffn_hidden)\n",
        "        self.linear2 = nn.Linear(ffn_hidden, embed_dim)\n",
        "\n",
        "        # LayerNorm + Dropout\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        # Self-attention with causal mask\n",
        "        attn_out, _ = self.self_attn(x, x, x, attn_mask=attn_mask)\n",
        "\n",
        "        # Residual + Norm\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "\n",
        "        # Feed-forward\n",
        "        ffn_out = self.linear2(F.gelu(self.linear1(x)))\n",
        "\n",
        "        # Residual + Norm\n",
        "        x = self.norm2(x + self.dropout(ffn_out))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len=512, embed_dim=768, num_heads=12, num_layers=12, ffn_hidden=3072, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # Token + Positional embeddings\n",
        "        # vocab_size means all the unique words\n",
        "        # max_len means size of the biggest sentence\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_emb = nn.Embedding(max_len, embed_dim)\n",
        "\n",
        "        # Transformer decoder layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            GPTBlock(embed_dim, num_heads, ffn_hidden, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)  # LM prediction head\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        B, L = input_ids.shape\n",
        "\n",
        "        # Token + Pos embeddings\n",
        "        tok_emb = self.token_emb(input_ids)\n",
        "        pos = torch.arange(L, device=input_ids.device).unsqueeze(0)\n",
        "        pos_emb = self.pos_emb(pos)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        # Create causal mask (prevent attending to future tokens)\n",
        "        # Shape: (L, L), with -inf above diagonal\n",
        "        attn_mask = torch.full((L, L), float('-inf'), device=input_ids.device)\n",
        "        attn_mask = torch.triu(attn_mask, diagonal=1)\n",
        "\n",
        "        # Pass through GPT blocks\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attn_mask)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        logits = self.lm_head(x)  # (B, L, vocab_size)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "bkT292WeFdq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "vocab_size = 30522\n",
        "model = MiniGPT(vocab_size=vocab_size, num_layers=2)  # small GPT-1\n",
        "\n",
        "input_ids = torch.randint(0, vocab_size, (3, 10))  # batch of 3 sentences, len=10\n",
        "logits = model(input_ids)\n",
        "\n",
        "print(logits.shape)  # (3, 10, 30522) â†’ next-token prediction\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FwLQ3d9Fg_g",
        "outputId": "8aeaa82c-d641-4bd9-d318-12e2c6833daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 10, 30522])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LelCIMGYFpMg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}